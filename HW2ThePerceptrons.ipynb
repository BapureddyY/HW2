{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYiZq0X2oB5t"
   },
   "source": [
    "# **CSCE 5218 / CSCE 4930 Deep Learning**\n",
    "\n",
    "# **HW1a The Perceptron** (20 pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vGVmKzgG2Ium",
    "outputId": "4cc2ca21-861a-4fba-a38c-83e3ec04bec8"
   },
   "outputs": [],
   "source": [
    "# Get the datasets\n",
    "train = !curl.exe --output train.dat http://huang.eng.unt.edu/CSCE-5218/train.dat\n",
    "test = !curl.exe --output test.dat http://huang.eng.unt.edu/CSCE-5218/test.dat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A69DxPSc8vNs",
    "outputId": "5440e602-8ecd-44cf-d48d-2e8b00cdcc52"
   },
   "outputs": [],
   "source": [
    "# Take a peek at the datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFXHLhnhwiBR"
   },
   "source": [
    "### Build the Perceptron Model\n",
    "\n",
    "You will need to complete some of the function definitions below.  DO NOT import any other libraries to complete this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "cXAsP_lw3QwJ"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "\n",
    "# Corpus reader, all columns but the last one are coordinates;\n",
    "#   the last column is the label\n",
    "def read_data(train):\n",
    "    f = open(train, 'r')\n",
    "\n",
    "    data = []\n",
    "    # Discard header line\n",
    "    f.readline()\n",
    "    for instance in f.readlines():\n",
    "        if not re.search('\\t', instance): continue\n",
    "        instance = list(map(int, instance.strip().split('\\t')))\n",
    "        # Add a dummy input so that w0 becomes the bias\n",
    "        instance = [-1] + instance\n",
    "        data += [instance]\n",
    "    return data\n",
    "\n",
    "\n",
    "def dot_product(array1, array2):\n",
    "    #TODO: Return dot product of array 1 and array 2\n",
    "#     if len(array1) != len(array2):\n",
    "#         raise ValueError(\"Arrays must have the same length\")\n",
    "\n",
    "    # Calculate dot product\n",
    "    dot_product_result = sum(x * y for x, y in zip(array1, array2))\n",
    "    \n",
    "    return dot_product_result\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    #TODO: Return outpout of sigmoid function on x\n",
    "    \n",
    "     return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# The output of the model, which for the perceptron is \n",
    "# the sigmoid function applied to the dot product of \n",
    "# the instance and the weights\n",
    "def output(weight, instance):\n",
    "    #TODO: return the output of the model \n",
    "    dot_product = np.dot(weight, instance)\n",
    "    return sigmoid(dot_product)\n",
    "\n",
    "# Predict the label of an instance; this is the definition of the perceptron\n",
    "# you should output 1 if the output is >= 0.5 else output 0\n",
    "def predict(weights, instance):\n",
    "    #TODO: return the prediction of the model\n",
    "    dot_product = np.dot(weights, instance)\n",
    "    output = sigmoid(dot_product)\n",
    "    return 1 if output >= 0.5 else 0\n",
    "\n",
    "\n",
    "# Accuracy = percent of correct predictions\n",
    "def get_accuracy(weights, instances):\n",
    "    # You do not to write code like this, but get used to it\n",
    "    correct = sum([1 if predict(weights, instances) == instance[-1] else 0\n",
    "                   for instance in instances])\n",
    "    return correct * 100 / len(instances)\n",
    "\n",
    "\n",
    "# Train a perceptron with instances and hyperparameters:\n",
    "#       lr (learning rate) \n",
    "#       epochs\n",
    "# The implementation comes from the definition of the perceptron\n",
    "#\n",
    "# Training consists on fitting the parameters which are the weights\n",
    "# that's the only thing training is responsible to fit\n",
    "# (recall that w0 is the bias, and w1..wn are the weights for each coordinate)\n",
    "#\n",
    "# Hyperparameters (lr and epochs) are given to the training algorithm\n",
    "# We are updating weights in the opposite direction of the gradient of the error,\n",
    "# so with a \"decent\" lr we are guaranteed to reduce the error after each iteration.\n",
    "def train_perceptron(instances, lr, epochs):\n",
    "\n",
    "    #TODO: name this step\n",
    "    weights = [0] * (len(instances[0])-1)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for instance in instances:\n",
    "            #TODO: name these steps\n",
    "            in_value = dot_product(weights, instance)\n",
    "            output = sigmoid(in_value)\n",
    "            error = instance[-1] - output\n",
    "            #TODO: name these steps\n",
    "            for i in range(0, len(weights)):\n",
    "                weights[i] += lr * error * output * (1-output) * instance[i]\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adBZuMlAwiBT"
   },
   "source": [
    "## Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "50YvUza-BYQF"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m instances_tr \u001b[38;5;241m=\u001b[39m read_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.dat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(instances_tr, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m instances_te \u001b[38;5;241m=\u001b[39m read_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest.dat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(instances_te)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:279\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(io_open)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_modified_open\u001b[39m(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m         )\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "instances_tr = read_data(\"train.dat\")\n",
    "f = open(instances_tr, 'r')\n",
    "instances_te = read_data(\"test.dat\")\n",
    "f = open(instances_te)\n",
    "lr = 0.005\n",
    "epochs = 5\n",
    "weights = train_perceptron(instances_tr, lr, epochs)\n",
    "accuracy = get_accuracy(weights, instances_te)\n",
    "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
    "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBXkvaiQMohX"
   },
   "source": [
    "## Questions\n",
    "\n",
    "Answer the following questions. Include your implementation and the output for each question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCQ6BEk1CBlr"
   },
   "source": [
    "\n",
    "\n",
    "### Question 1\n",
    "\n",
    "In `train_perceptron(instances, lr, epochs)`, we have the follosing code:\n",
    "```\n",
    "in_value = dot_product(weights, instance)\n",
    "output = sigmoid(in_value)\n",
    "error = instance[-1] - output\n",
    "```\n",
    "\n",
    "Why don't we have the following code snippet instead?\n",
    "```\n",
    "output = predict(weights, instance)\n",
    "error = instance[-1] - output\n",
    "```\n",
    "\n",
    "#### TODO Add your answer here (text only)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer: In the context of training a perceptron, the goal is to update the weights based on the error between the predicted output and the actual output. \n",
    "\n",
    "The code you provided suggests using the `predict` function to obtain the output and then calculating the error as the difference between the actual output (stored in `instance[-1]`) and the predicted output. However, this approach assumes that the `predict` function returns the actual output of the perceptron (0 or 1), which might not be the case.\n",
    "\n",
    "Usually, the `predict` function returns the predicted output of the perceptron after passing the dot product of weights and instances through a sigmoid function, which squashes the output to a value between 0 and 1. This value represents the probability that the instance belongs to a certain class. In contrast, the actual output used in error calculation is a binary value (0 or 1).\n",
    "\n",
    "Therefore, using the output directly from the `predict` function for error calculation might not be appropriate. Instead, you should calculate the output using the dot product of weights and instances, pass it through the sigmoid function to get a probability, and then calculate the error based on the difference between the actual output and this probability.\n",
    "\n",
    "Here's a more detailed explanation of the code you provided:\n",
    "\n",
    "1. `in_value = dot_product(weights, instance)`: This computes the dot product of weights and instance features.\n",
    "\n",
    "2. `output = sigmoid(in_value)`: This computes the output of the perceptron by passing the dot product through the sigmoid function, which squashes the output to a value between 0 and 1.\n",
    "\n",
    "3. `error = instance[-1] - output`: This calculates the error as the difference between the actual output (`instance[-1]`) and the predicted output (`output`). This error is then used to update the weights during training.\n",
    "\n",
    "Using the output from the `predict` function directly for error calculation would skip the sigmoid transformation and might lead to incorrect error estimation. Therefore, it's better to use the dot product followed by the sigmoid function to calculate the output for error calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JU3c3m6YL2rK"
   },
   "source": [
    "### Question 2\n",
    "Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.\n",
    "\n",
    "```\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
    "lr = [0.005, 0.01, 0.05]              # learning rate\n",
    "```\n",
    "\n",
    "TODO: Write your code below and include the output at the end of each training loop (NOT AFTER EACH EPOCH)\n",
    "of your code.The output should look like the following:\n",
    "```\n",
    "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "[and so on for all the combinations]\n",
    "```\n",
    "You will get different results with different hyperparameters.\n",
    "\n",
    "#### TODO Add your answer here (code and output in the format above) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 44\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Assuming you have a dataset named 'data' where each row is an instance, \u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# and the last column contains the target labels.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Split the dataset into training and testing sets\u001b[39;00m\n\u001b[0;32m     43\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)  \u001b[38;5;66;03m# for reproducibility\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle(data)\n\u001b[0;32m     45\u001b[0m num_instances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[0;32m     46\u001b[0m test_set_size \u001b[38;5;241m=\u001b[39m num_instances \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# Assuming 1/3 of the data is used for testing\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the perceptron functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def predict(weights, instance):\n",
    "    dot_product = np.dot(weights, instance)\n",
    "    output = sigmoid(dot_product)\n",
    "    return 1 if output >= 0.5 else 0\n",
    "\n",
    "def train_perceptron(instances, lr, epochs):\n",
    "    # Initialize weights\n",
    "    num_features = len(instances[0]) - 1\n",
    "    weights = np.zeros(num_features)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        for instance in instances:\n",
    "            features = instance[:-1]\n",
    "            target = instance[-1]\n",
    "            output = predict(weights, features)\n",
    "            error = target - output\n",
    "            weights += lr * error * features\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def test_perceptron(weights, test_set):\n",
    "    num_correct = 0\n",
    "    for instance in test_set:\n",
    "        features = instance[:-1]\n",
    "        target = instance[-1]\n",
    "        prediction = predict(weights, features)\n",
    "        if prediction == target:\n",
    "            num_correct += 1\n",
    "    accuracy = num_correct / len(test_set)\n",
    "    return accuracy\n",
    "\n",
    "# Assuming you have a dataset named 'data' where each row is an instance, \n",
    "# and the last column contains the target labels.\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "np.random.seed(42)  # for reproducibility\n",
    "np.random.shuffle(data)\n",
    "num_instances = len(data)\n",
    "test_set_size = num_instances // 3  # Assuming 1/3 of the data is used for testing\n",
    "\n",
    "# Test for different hyperparameters\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]      # number of epochs\n",
    "lr = [0.005, 0.01, 0.05]               # learning rate\n",
    "\n",
    "for tr_p in tr_percent:\n",
    "    for epochs in num_epochs:\n",
    "        for learning_rate in lr:\n",
    "            # Split data into training and testing sets\n",
    "            num_train_instances = int(tr_p / 100 * num_instances)\n",
    "            train_data = data[:num_train_instances]\n",
    "            test_data = data[num_train_instances:num_train_instances + test_set_size]\n",
    "            \n",
    "            # Train the perceptron\n",
    "            weights = train_perceptron(train_data, learning_rate, epochs)\n",
    "            \n",
    "            # Test the perceptron\n",
    "            accuracy = test_perceptron(weights, test_data)\n",
    "            \n",
    "            print(f\"Training Percent: {tr_p}%, Epochs: {epochs}, Learning Rate: {learning_rate}, Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "G-VKJOUu2BTp"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Arrays must be of equal length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m   size \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mlen\u001b[39m(instances_tr)\u001b[38;5;241m*\u001b[39mtr_size\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     11\u001b[0m   pre_instances \u001b[38;5;241m=\u001b[39m instances_tr[\u001b[38;5;241m0\u001b[39m:size]\n\u001b[1;32m---> 12\u001b[0m   weights \u001b[38;5;241m=\u001b[39m train_perceptron(pre_instances, lr, epochs)\n\u001b[0;32m     13\u001b[0m   accuracy \u001b[38;5;241m=\u001b[39m get_accuracy(weights, instances_te)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#tr: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pre_instances)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, epochs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m3\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, learning rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy (test, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(instances_te)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instances): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[32], line 87\u001b[0m, in \u001b[0;36mtrain_perceptron\u001b[1;34m(instances, lr, epochs)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m instance \u001b[38;5;129;01min\u001b[39;00m instances:\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;66;03m#TODO: name these steps\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m         in_value \u001b[38;5;241m=\u001b[39m dot_product(weights, instance)\n\u001b[0;32m     88\u001b[0m         output \u001b[38;5;241m=\u001b[39m sigmoid(in_value)\n\u001b[0;32m     89\u001b[0m         error \u001b[38;5;241m=\u001b[39m instance[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m output\n",
      "Cell \u001b[1;32mIn[32], line 26\u001b[0m, in \u001b[0;36mdot_product\u001b[1;34m(array1, array2)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdot_product\u001b[39m(array1, array2):\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m#TODO: Return dot product of array 1 and array 2\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(array1) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(array2):\n\u001b[1;32m---> 26\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArrays must be of equal length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Initialize the dot product to zero\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Arrays must be of equal length"
     ]
    }
   ],
   "source": [
    "instances_tr = read_data(\"train.dat\")\n",
    "instances_te = read_data(\"test.dat\")\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]     # number of epochs\n",
    "lr_array = [0.005, 0.01, 0.05]        # learning rate\n",
    "\n",
    "for lr in lr_array:\n",
    "  for tr_size in tr_percent:\n",
    "    for epochs in num_epochs:\n",
    "      size =  round(len(instances_tr)*tr_size/100)\n",
    "      pre_instances = instances_tr[0:size]\n",
    "      weights = train_perceptron(pre_instances, lr, epochs)\n",
    "      accuracy = get_accuracy(weights, instances_te)\n",
    "    print(f\"#tr: {len(pre_instances):0}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
    "            f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFB9MtwML24O"
   },
   "source": [
    "### Question 3\n",
    "Write a couple paragraphs interpreting the results with all the combinations of hyperparameters. Drawing a plot will probably help you make a point. In particular, answer the following:\n",
    "- A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
    "- B. How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
    "   ```\n",
    "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
    "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "```\n",
    "- C. Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?\n",
    "- D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
    "\n",
    "#### TODO: Add your answer here (code and text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38rA_Kp3wiBX"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW2_The_Perceptron.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
